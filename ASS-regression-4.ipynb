{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc6c7a6-35f5-413e-bc5d-806a440ac5a6",
   "metadata": {},
   "source": [
    "Q1.What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that adds a penalty term to the ordinary least squares (OLS) objective function. This penalty term is the absolute value of the coefficients multiplied by a tuning parameter (λ). Lasso aims to reduce the magnitude of coefficient values and, more importantly, force some coefficients to become exactly zero. This makes Lasso useful for both regularization and feature selection. It differs from other regression techniques like Ridge Regression and OLS by its use of the L1 penalty, which promotes sparsity in the model.\n",
    "\n",
    "Q2.What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "The main advantage of Lasso Regression in feature selection is that it can automatically perform variable selection by driving some coefficients to exactly zero. This means it can identify and exclude irrelevant or redundant features from the model, leading to a simpler and more interpretable model that is less prone to overfitting.\n",
    "\n",
    "Q3.How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "Interpreting the coefficients in Lasso Regression is similar to interpreting coefficients in ordinary linear regression. Larger coefficients still indicate stronger relationships between the predictor and the response variable. Coefficients that are exactly zero indicate features that have been excluded from the model. The sign and magnitude of the non-zero coefficients indicate the direction and strength of the relationships, respectively.\n",
    "\n",
    "Q4.What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "The main tuning parameter in Lasso Regression is the regularization parameter λ (lambda). It controls the amount of shrinkage applied to the coefficients. Larger values of λ lead to more aggressive shrinking of coefficients, potentially resulting in more features having coefficients set to zero. Smaller values of λ allow coefficients to remain closer to their original values. The choice of λ is crucial, and it can be determined using techniques like cross-validation.\n",
    "\n",
    "Q5.Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "Lasso Regression itself is a linear regression technique and is suitable for linear relationships between variables. However, it can be extended to handle non-linear regression problems by using transformations of input features. You can introduce polynomial features, interactions, or other non-linear transformations of the original features and then apply Lasso Regression to this augmented feature space. Keep in mind that this approach might increase the complexity of the model.\n",
    "\n",
    "Q6.What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "Ans:-\n",
    "\n",
    "The main difference between Ridge Regression and Lasso Regression lies in the type of penalty applied to the coefficients:\n",
    "Ridge Regression adds an L2 penalty, which is the sum of squared coefficients.\n",
    "Lasso Regression adds an L1 penalty, which is the sum of absolute values of coefficients.\n",
    "\n",
    "As a result, Ridge Regression tends to shrink all coefficients towards zero without necessarily driving any coefficients to exactly zero. Lasso Regression, on the other hand, can perform feature selection by driving some coefficients to zero.\n",
    "\n",
    "Q7. **Can Lasso Regression handle multicollinearity in the input features? If yes, how?**\n",
    "Yes, Lasso Regression can handle multicollinearity to some extent. Multicollinearity occurs when predictor variables are highly correlated, which can lead to unstable coefficient estimates. Lasso's L1 penalty can drive one of the correlated variables to zero, effectively choosing one variable over the other and helping in dealing with multicollinearity.\n",
    "\n",
    "Q8. **How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?**\n",
    "Choosing the optimal value of λ in Lasso Regression is often done through cross-validation. You would typically perform k-fold cross-validation, where you split your dataset into training and validation sets multiple times. For each fold, you fit the Lasso Regression model with different values of λ, and then choose the value that yields the best cross-validation performance metric, such as the lowest mean squared error. This helps you find the right balance between fitting the data and regularizing the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
